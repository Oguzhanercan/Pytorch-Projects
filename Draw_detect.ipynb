{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drawing a number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we run this cell we will be able to draw a shape.We are drawing a number (0,9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "drawing = False\n",
    "showing = 1\n",
    "new_image = np.zeros((500,500,3),np.float32)\n",
    "\n",
    "def draw(event,x,y,flags,param):\n",
    "    \n",
    "    global showing,drawing,new_image\n",
    "    \n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        drawing = True\n",
    "    \n",
    "    elif event == cv2.EVENT_MOUSEMOVE:\n",
    "        if drawing == True:\n",
    "            cv2.circle(img,center = (x,y), radius = 8 , color = (255,255,0), thickness = -1) # if thickness == 1 it fill up\n",
    "            \n",
    "    elif event == cv2.EVENT_LBUTTONUP:\n",
    "        drawing = False\n",
    "        \n",
    "    elif event == cv2.EVENT_RBUTTONDOWN:\n",
    "        new_image = img\n",
    "        showing = 0\n",
    "        \n",
    "        return new_image\n",
    "        \n",
    "cv2.namedWindow(winname = \"draw\")\n",
    "cv2.setMouseCallback(\"draw\",draw)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "img = np.zeros((500,500,3),np.float32)\n",
    "\n",
    "\n",
    "\n",
    "while showing == 1: \n",
    "    cv2.imshow(\"draw\",img)\n",
    "    if cv2.waitKey(20) & 0xFF == 27:\n",
    "        \n",
    "        break\n",
    "        \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1bf906f01d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(new_image)\n",
    "#The shape we have drawn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1bf9072bbe0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resized_img = cv2.resize(new_image,(28,28))\n",
    "resized_img = cv2.cvtColor(resized_img, cv2.COLOR_BGR2GRAY)\n",
    "plt.imshow(resized_img)\n",
    "#we have resized the image because shape of our dataset(mnist) is 28x28. Then we have converted the color to gray from BGR(blue,green,red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([28, 28])\n"
     ]
    }
   ],
   "source": [
    "resized_img = torch.from_numpy(resized_img)\n",
    "\n",
    "print(type(resized_img))\n",
    "print(resized_img.shape)\n",
    "#we have converted the type of image from numpy array to pytorch tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading and preparing dataset (mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "mean = 0.1307\n",
    "std = 0.3081\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Normalize((mean,),\n",
    "                                                            (std,))])\n",
    "\n",
    "trainset = datasets.MNIST(\"MNIST_data/\", download = True , train = True ,transform = transform)\n",
    "testset = datasets.MNIST(\"MNIST_data/\", download = True , train = False ,transform = transform)\n",
    "\n",
    "\n",
    "\n",
    "indices = list(range(len(trainset)))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(0.2 * len(trainset))) #0.2 validation size\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset,batch_size = 64,sampler = train_sampler)\n",
    "validloader = torch.utils.data.DataLoader(trainset,batch_size = 64,sampler = valid_sampler)\n",
    "testloader = torch.utils.data.DataLoader(testset,batch_size = 64, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building The Neural Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two diffrent types of neural network below. Inside of the comment block the model is that fully connected neural network.\n",
    "The actual model is CNN (convolutional Neural Network). We should use CNN because it performs better than fully connected neural network on image dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"model = nn.Sequential(nn.Linear(784,128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Dropout(0.2),\n",
    "                     nn.Linear(128,64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Dropout(0.1),\n",
    "                      nn.Linear(64,10),\n",
    "                      nn.LogSoftmax(dim = 1))\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(),lr = 0.01)\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\"\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, 3, padding=1)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(8, 16, 3, padding=1)\n",
    "        \n",
    "        self.pool =nn.MaxPool2d(2,2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(7*7*16,256)\n",
    "        \n",
    "        self.fc2 = nn.Linear(256,64)\n",
    "        \n",
    "        self.fc3 = nn.Linear(64,10)\n",
    "        \n",
    "        self.BN1 = nn.BatchNorm2d(8)\n",
    "        \n",
    "        self.BN2 = nn.BatchNorm2d(16)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        \n",
    "        x = self.pool(F.relu(self.BN1(self.conv1(x))))\n",
    "        \n",
    "        x = self.pool(F.relu(self.BN2(self.conv2(x))))\n",
    " \n",
    "        x = x.view(-1, 7 * 7 * 16)\n",
    "    \n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        x = F.relu(self.fc3(x))\n",
    "        \n",
    "        return x\"\"\"\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "            super(CNN,self).__init__()\n",
    "            \n",
    "            self.cnn1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, stride=1, padding=1)\n",
    "            self.batch_norm1 = nn.BatchNorm2d(8) # feature map = 8\n",
    "        \n",
    "            self.cnn2 = nn.Conv2d(in_channels=8, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "           \n",
    "            self.batch_norm2 = nn.BatchNorm2d(32) #feature map = 32\n",
    "            \n",
    "            self.fc1 = nn.Linear(7*7*32,600)\n",
    "            self.fc2 = nn.Linear(600,10)\n",
    "            \n",
    "            self.relu = nn.ReLU()\n",
    "            self.maxpool = nn.MaxPool2d(kernel_size = 2)\n",
    "\n",
    "    def forward(self,x):\n",
    "            out = self.cnn1(x)\n",
    "            out = self.batch_norm1(out)\n",
    "            out = self.relu(out)\n",
    "            out = self.maxpool(out)\n",
    "            out = self.cnn2(out)\n",
    "            out = self.batch_norm2(out)\n",
    "            out = self.relu(out)\n",
    "            out = self.maxpool(out)\n",
    "            # flattening:\n",
    "            out = out.view(-1,1568)\n",
    "            out = self.fc1(out)\n",
    "            out = self.relu(out)\n",
    "            out = self.fc2(out)\n",
    "            return out\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = CNN().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model on the training set and choosing the best parameters by using the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again there are 2 different types of way to train the models. One of them is for fully connected neural network and the other one is for CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.450298 \tValidation Loss: 0.090914\n",
      "Validation loss decreased (inf --> 0.090914).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.070974 \tValidation Loss: 0.095294\n",
      "Epoch: 3 \tTraining Loss: 0.051506 \tValidation Loss: 0.062060\n",
      "Validation loss decreased (0.090914 --> 0.062060).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.043930 \tValidation Loss: 0.051119\n",
      "Validation loss decreased (0.062060 --> 0.051119).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.036472 \tValidation Loss: 0.056859\n",
      "Epoch: 6 \tTraining Loss: 0.034323 \tValidation Loss: 0.062366\n",
      "Epoch: 7 \tTraining Loss: 0.031190 \tValidation Loss: 0.068888\n",
      "Epoch: 8 \tTraining Loss: 0.027700 \tValidation Loss: 0.059696\n",
      "Epoch: 9 \tTraining Loss: 0.026937 \tValidation Loss: 0.062949\n",
      "Epoch: 10 \tTraining Loss: 0.026725 \tValidation Loss: 0.089331\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"epochs = 6\n",
    "\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "for e in range(epochs):\n",
    "    train_loss = 0\n",
    "    valid_loss = 0\n",
    "    \n",
    "    model.train()\n",
    "    for images,labels in trainloader:\n",
    "        \n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        log_ps = model(images)\n",
    "\n",
    "        loss = criterion(log_ps,labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    model.eval() \n",
    "    for images, labels in validloader:\n",
    "        \n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        images = images.view(images.shape[0], -1)\n",
    "\n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        valid_loss += loss.item()\n",
    "        \n",
    "    \n",
    "    train_loss = train_loss/len(trainloader)\n",
    "    valid_loss = valid_loss/len(validloader)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        e+1, \n",
    "        train_loss,\n",
    "        valid_loss\n",
    "        ))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "        valid_loss_min = valid_loss\"\"\"\n",
    "\n",
    "n_epochs = 10\n",
    "\n",
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "for epoch in range(1, n_epochs+1):\n",
    "\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train()\n",
    "    for data, target in trainloader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "       \n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    for data, target in validloader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        \n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(trainloader.sampler)\n",
    "    valid_loss = valid_loss/len(validloader.sampler)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'model.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model.pt'))# saving the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the number in the picture we draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28])\n",
      "tensor([[3]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    resized_img = resized_img.to(device)\n",
    "    resized_img  = resized_img.view(1,1,28,28)\n",
    "    print(resized_img.shape)\n",
    "    \n",
    "    output = model(resized_img)\n",
    "\n",
    "    ps = torch.exp(output)\n",
    "    top_p, top_class = ps.topk(1, dim=1)\n",
    "    print(top_class)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
